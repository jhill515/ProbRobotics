{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notes are meant to be brief as this material is covered extensively by other notes.\n",
    "\n",
    "# Gaussian Distribution(s)\n",
    "Otherwise known as the **Error PDF**, it is described by the following equations:\n",
    "\n",
    "Single dimension of $x$\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot \\exp\\bigg(\\frac{-(x-\\mu)^2}{2\\sigma^2} \\bigg)\n",
    "$$\n",
    "\n",
    "Therefore...\n",
    "$$\n",
    "P(a < x < b) = \\int_a^b{p(x)dx}\n",
    "$$\n",
    "\n",
    "Multi-dimensional vector $X=<x_1 ... x_n> \\in \\mathbb{R}^n$:\n",
    "$$\n",
    "p(X) = \\frac{1}{\\sqrt{2\\pi\\det(\\Sigma)}} \\cdot \\exp\\bigg( -\\frac{1}{2} (X-\\mu)^\\intercal \\Sigma^{-1} (X-\\mu) \\bigg)\n",
    "$$\n",
    "\n",
    "Therefore...\n",
    "$$\n",
    "P(a_1 < x_1 < b_1, ..., a_n < x_n < b_n) = \\int_{a_1}^{b_1}...\\int_{a_n}^{b_n} p(X) dx_1...dx_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditionally Joint Distributions\n",
    "This is in reference to joint distributions such as *what is the probability of X=x when Y=y?*\n",
    "\n",
    "By counting occurances in a sample, we can compute the such a probability as:\n",
    "$$\n",
    "p(x|y) = \\frac{p(x,y)}{p(y)}\n",
    "$$\n",
    "\n",
    "Sometimes, it is not possible (or impractical) to count samples; we can employ *Bayes Rule* in such circumstances:\n",
    "$$\n",
    "p(x|y) = \\frac{p(y|x) \\cdot p(x)}{p(y)}\n",
    "$$\n",
    "\n",
    "It's important to bear in mind the implication, as this is the mathematical basis of inference: the denominator does not depend upon *x* at all, and rather is a *normalization* of $p(y|x) \\cdot p(x)$. Because of this, we can typically see the following notation:\n",
    "$$\n",
    "p(x|y) = \\eta p(y|x) \\cdot p(x) \\quad \\text{where} \\quad \\eta = \\frac{1}{p(y)}\n",
    "$$\n",
    "\n",
    "If *x* is the parameter we wish to infer, then $p(x)$ is called the **prior probability distribution** while *y* would be treated as **data**. Thus the calculated $p(x|y)$ is called the **posterior probability distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation & Moments\n",
    "**Expected value** is the estimated value of an arbitrary measurement of the random variable *X*. This is sometimes confused with *mean* or *median*, and though they are related, those two meanings pertain only when the probability distribution is truly unknown.\n",
    "\n",
    "In this case, expected value is calculated as:\n",
    "$$\n",
    "E[X] = \\sum_x x \\cdot p(x) \\quad \\text{(discrete)} \\\\\n",
    "E[X] = \\int_x x \\cdot p(x) dx \\quad \\text{(continuous)}\n",
    "$$\n",
    "\n",
    "Considering *expected value* as an operator, it is a linear operator. That is:\n",
    "$$\n",
    "E[aX + b] = aE[X] + b\n",
    "$$\n",
    "\n",
    "Every probability distribution has a unique family of statistics called **moments**; these are also computed with the *expected value* operator. In general, we define the *first moment* as $E[X]$, also called $E_1[X]$. The *second moment* is defined as follows:\n",
    "$$\n",
    "E_2[X] = E[X - E[X]]^2 = E[X^2] - E[X]^2\n",
    "$$\n",
    "\n",
    "If the first moment is considered to be the *mean*, the second is the *variance*. Subsequent moments are computed as follows:\n",
    "$$\n",
    "E_n[X] = E[X - E[X]]^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "Probability is inherently tied to *Information Theory*, the study of how to quantize and effectively use information on a mathematical level. One key measure of how useful an event is is a measurement called **entropy**. For simplicity, we'll compute it as:\n",
    "$$\n",
    "H_p(x) = E[-\\log_2 p(x)]\n",
    "$$\n",
    "\n",
    "Which resolves to:\n",
    "$$\n",
    "H_p(x) = -\\sum_x p(x) \\log_2 p(x) \\quad \\text{(discrete)} \\\\\n",
    "H_p(x) = -\\int_x p(x) \\log_2 p(x) dx \\quad \\text{(continuous)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Generative Laws & Belief\n",
    "In general, we model the **state** of the environment and its transitions as a *stochastic process* (that is, a sequence of random variables), particularly as a *Markov Process*. What do we mean by that? The state of the environment at any given time, $x_t$, *depends* upon all previous states and the actions taken by the robot. The actions taken by the robot depend on sensor observations $z_{1:t-1}$, which in turn also depend upon the aforementioned state. Through convention, however, we assume that the action ${u_t}$ occurs just before the state observation $u_t$.\n",
    "\n",
    "We define the state to be **complete** when it contains sufficient information to extrapolate all previous states. This Markovian condition allows for constraining the control problem:\n",
    "\n",
    "$$\n",
    "p(x_t | x_{0:t}, z_{1:t-1}, u_{1:t}) = p(x_t | x_{t-1}, u_t) \\\\\n",
    "p(z_t | x_{0:t}, z_{1:t-1}, u_{1:t}) = p(z_t | x_t)\n",
    "$$\n",
    "\n",
    "Graphically, this model can be depicted as follows:\n",
    "\n",
    "![Dynamic Bayes Net of Generative Model](images/DBN_GenModel.png)\n",
    "###### *Figure 1: Dynamic Bayes net of a Generative Belief Model of State*\n",
    "\n",
    "Note that in the image there is a dotted-arrow pointing from each sensor measurement to a subsequent action. This represents the causal model of our robot reacting to sensory data (we don't want a truly *random* action selection!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
