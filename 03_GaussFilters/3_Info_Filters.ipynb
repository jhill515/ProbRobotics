{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Information Filter\n",
    "\n",
    "Otherwise known as the *Kalman Dual*, the *Information Filter* (IF) is based upon the **canonical representation** of the Gaussian function --\n",
    "\n",
    "*Singular Dimension*:\n",
    "$$\n",
    "\\omega = \\sigma^{-2} \\\\\n",
    "\\xi = \\sigma^{-2} \\mu \\\\\n",
    "p(x) = \\eta e^{-\\frac{1}{2}x^2 \\omega+ \\xi x}\n",
    "$$\n",
    "\n",
    "*Multi-dimensional*:\n",
    "$$\n",
    "\\Omega = \\Sigma^{-1} \\\\\n",
    "\\xi = \\Sigma^{-1} \\mu \\\\\n",
    "p(x) = \\eta e^{-\\frac{1}{2}x^\\top \\Omega x + x^\\top \\xi}\n",
    "$$\n",
    "\n",
    "Notice the difference in this form: the exponential is represented as a quadratic equation. This can allow us alternative representations when we're faced with numerical accuracy constraints. For example, when faced with multiplying two R.V.s, we can use the logarithmic approach:\n",
    "\n",
    "$$\n",
    "p(x) \\cdot p(y) \\rightarrow \\ln(p(x) \\cdot p(y)) = \\eta_{p(x)} \\eta_{p(y)} \\cdot \\left(-\\frac{1}{2}x^\\top \\Omega_{p(x)} x + x^\\top \\xi_{p(x)} + -\\frac{1}{2}y^\\top \\Omega_{p(y)} y + y^\\top \\xi_{p(y)} \\right)\n",
    "$$\n",
    "\n",
    "And returning back to our standard representations of $\\mu$ and $\\Sigma$, we just invert $\\Omega$:\n",
    "$$\n",
    "\\Sigma = \\Omega^{-1} \\\\\n",
    "\\mu = \\Omega^{-1} \\xi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IF Algorithm\n",
    "The algorithm still assumes that both state transition and sensor measurement are linear functions. That is:\n",
    "$$\n",
    "x_t = Ax_{t-1} + B_t u_t + \\epsilon_t \\\\\n",
    "z_t = C_t x_t + \\delta_t\n",
    "$$\n",
    "\n",
    "**InformationFilter**($\\xi_{t-1}, \\Omega_{t-1}, u_t, z_t$):\n",
    "* Predict $\\xi_t , \\Omega_t$\n",
    "  * $ \\overline{\\Omega}_t = (A_t\\Omega_{t-1}^{-1} A_t^\\top + R_t)^{-1} $\n",
    "  * $ \\overline{\\xi}_t = \\overline{\\Omega}_t(A_t \\Omega_{t-1}^{-1} \\xi_{t-1} + B_t u_t) $\n",
    "* Correct estimate of $\\xi_t , \\Omega_t$\n",
    "  * $ \\Omega_t = C_t^\\top Q_t^{-1} C_t + \\overline{\\Omega}_t $\n",
    "  * $ \\xi_t = C_t^\\top Q_t^{-1} z_t + \\overline{\\xi}_t $\n",
    "* Return $\\xi_t, \\Omega_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Extended Information Filter\n",
    "Just like EKF, there is an EIF following the same linearization strategy as before. *Unfortunately*, we cannot exercise this extenstion using solely the *information vector* $\\xi_x$ and *precision matrix* $\\Omega_x$ -- we need to transform back to $\\mu_x$ to apply the non-linear transform function $g(x)$:\n",
    "\n",
    "**ExtendedInformationFilter**($\\xi_{t-1}, \\Omega_{t-1}, u_t, z_t$):\n",
    "* Predict $\\xi_t , \\Omega_t$\n",
    "  * $ \\mu_{t-1} = \\Omega_{t-1}^{-1} \\xi_{t-1} $\n",
    "  * $ \\overline{\\mu}_t = g(u_t, \\mu_{t-1}) $\n",
    "  * $ \\overline{\\Omega}_t = (G_t\\Omega_{t-1}^{-1} G_t^\\top + R_t)^{-1} $\n",
    "  * $ \\overline{\\xi}_t = \\overline{\\Omega}_t \\overline{\\mu}_t $\n",
    "* Correct estimate of $\\xi_t , \\Omega_t$\n",
    "  * $ \\Omega_t = \\overline{\\Omega}_t + H_t^\\top Q_t^{-1} H_t $\n",
    "  * $ \\xi_t = \\overline{\\xi}_t + H_t^\\top Q_t^{-1} \\left[ z_t - h(\\overline{\\mu}_t) + H_t \\overline{\\mu}_t \\right] $\n",
    "* Return $\\xi_t, \\Omega_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Things Considered\n",
    "As in all things engineering, there are advantages and disadvantages to this approach...\n",
    "\n",
    "* It's really easy to represent universal uncertainty: just set $\\Omega = 0$, but then all of the equations break down (i.e., can't invert the zero-matrix)!\n",
    "* IF is more computationally stable than KF. With a little bit of tweeking, this is immensely helpful when dealing with exceptionally large state vectors (i.e., 100s of variables)\n",
    "* When applying multiple sources of decentralized information, the logarithmic form is very stable.\n",
    "* The inversion of the precision matrix for EIF *sucks*, especially when dealing with high-dimensional state spaces. Because of this, EKF is favored.\n",
    "* The precision matrix can be thought of as a graph of a *Markov random field*; sparseness within that matrix connotes sparseness within the graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
