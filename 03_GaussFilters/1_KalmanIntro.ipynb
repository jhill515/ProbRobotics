{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Filters Prelude\n",
    "As noted in the previous chapter's [notes](../02_RecursiveStateEst/2_BayesFilterIntro.ipynb), each family of Bayesian Filters have characteristics which make them advantageous at times, but at the cost of introducing some bias and accepting other limitations. Here, we'll explore *Gaussian Filters* -- these are filters which assume that each input dimension is represented by a Gaussian (Normal) Distribution. As a brief reminder, this means that for the probability distribution of a random vector, $X$, is represented as follows:\n",
    "\n",
    "$$\n",
    "p(X) = \\frac{1}{\\sqrt{(2\\pi)^k \\left| \\Sigma \\right|}} \\exp \\left({-\\frac{1}{2} (X-\\mu)^\\top \\Sigma^{-1} (X-\\mu)} \\right) \\: \\text{where} \\: X \\in \\mathbb{R}^k\n",
    "$$\n",
    "\n",
    "This carries a few consequences:\n",
    "* Each dimension is continuous and infinite\n",
    "* Each density is unimodal (that is, there is a single peak in the distribution)\n",
    "* Each density is symmetric\n",
    "* The *mean* and *covariance* are the ONLY moments of the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Origins and Criteria of the Kalman Filter\n",
    "This technique was invented by Swerling (1958) and Kalman (1960) to both filter and create preditions of *linear systems* with Gaussian errors. In essence, for each moment in time, belief (posterior) is represented by both mean $\\mu_t$ and covariance $\\Sigma_t$. For this to be valid, three conditions must be met:\n",
    "\n",
    "**1. State Transition $p(x_t \\: | \\: u_t, x_{t-1})$ must be a linear transformation.**\n",
    "Particularly, even though we're treating this as a conditional probability, the reality is this implies that we can extrapolate $x_{t-1}$ to $x_t$ with noise $u_t$. This implies that a *state model* is known *a priori*. This is best summarized as:\n",
    "\n",
    "$$\n",
    "x_t = Ax_{t-1} + u_t\n",
    "$$\n",
    "\n",
    "**2. The Measurement Distribution $p(z_t \\: | \\: x_t)$ must also be a linear transformation.**\n",
    "Like before, we're expecting that sensor measurements can be derived from the state with noise. Note, sometimes sensors cannot perfectly tell us every dimension of the state vector we're estimating (for example, an IMU cannot give exact geographic coordinates, but it can tell you velocity and higher-order differental vectors). We'll summarize this model as:\n",
    "\n",
    "$$\n",
    "z_t = C_t x_t + \\delta_t\n",
    "$$\n",
    "\n",
    "**3. The Initial (*a priori*) Belief must be Gaussian Distributed.**\n",
    "While this seems obvious, it might be difficult to model. Sometimes we have perfect knowledge of the initial state (such as when the system starts, we're at known coordinates), and sometimes there's very little certainty of a distribution (for instance, we're told by another system \\[or person\\] information before the phenomenon has crossed the sensor horizon). The confusing part is leveraging exceptionally small or large covariances with the initial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
