{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Happens When Transitions are Non-Linear?\n",
    "In the previous notes, one of the fundamental assumptions of the Kalman filter is that both state transition and sensor states are linear functions of the current state. This... is problematic: almost **ALL** systems are inherently non-linear! Let's look at an example non-linear transformation:\n",
    "\n",
    "<img src=\"images/NonLinearExample.jpg\" alt=\"Non-Linear Transform Example from Textbook\" width=\"400px\" />\n",
    "\n",
    "Let's assume that $x$ is a Gaussian R.V., and it is transformed via some non-linear function resulting in $y=g(x)$. The resultant distribution of $y$ will **not** be Gaussian! In these cases, we can still *approximate* the resultant distribution into the form of a Gaussian through many different methods. Simply, we could run a Monte-Carlo experiment of draws of $x$ and get a large dataset of $y$ values. Estimating the first two moments from the dataset (that is, computing $\\mu_y, \\sigma_y^2$), we could simply use those as the parameters for a Gaussian $\\hat{y} \\sim N(\\mu_y, \\sigma_y)$.\n",
    "\n",
    "Another method is to apply a linearization technique to the transform..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Extended Kalman Filter\n",
    "The key here is to linearize the transform via *[Taylor Expansion](https://en.wikipedia.org/wiki/Taylor_series#Definition)* to a first-order approximation; that is to say:\n",
    "\n",
    "$$\n",
    "g(x) \\approx g(a) + g'(a)(x-a)\n",
    "$$\n",
    "\n",
    "One issue faced with this approximation is the virtue that the slope of this linearization is completely dependent upon the value $a$ we select. A logical conclusion is to set $a = \\mu_{t-1}$, which adjusts our transformation to\n",
    "\n",
    "$$\n",
    "g(u_t, x_{t-1}) \\approx g(u_t, u_{t-1}) + G_t (x_{t-1} - \\mu_{t-1}) \\\\\n",
    "G_t = g'(u_t, u_{t-1}) := \\frac{\\partial g(u_t, x_{t-1})}{\\partial x_{t-1}}\n",
    "$$\n",
    "\n",
    "Note that $G_t$ is the *Jacobian* of our transformation when $x \\in \\mathbb{R}^n$. Given this matrix, we can linearlly transform the covariance (note, this is still an approximation), and thus only slightly modify our original Kalman algorithm.\n",
    "\n",
    "\n",
    "## The Extended Kalman Algorithm\n",
    "**ExtendedKalmanFilter**($\\mu_{t-1}, \\Sigma_{t-1}, u_t, z_t$):\n",
    "* Predict $\\mu_t, \\Sigma_t$\n",
    "  * $\\overline{\\mu_t} = g(u_t, \\mu_{t-1})$\n",
    "  * $\\overline{\\Sigma_t} = G_t \\Sigma_{t-1} G_t^\\top + R_t$\n",
    "* Correct estimate of $\\mu_t, \\Sigma_t$\n",
    "  * $K_t = \\overline{\\Sigma_t} H_t^\\top (H_t \\overline{\\Sigma_t} H_t^\\top + Q_t)^{-1}$\n",
    "  * $\\mu_t = \\overline{\\mu_t} + K_t \\cdot (z_t - h(\\mu_t))$\n",
    "  * $\\Sigma_t = (I - K_t H_t) \\overline{\\Sigma_t}$\n",
    "  \n",
    "Note that $H_t$ may also be the Jacobian of the transform $z_t = h(x_t)$. That is to say either transform can remain linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Unscented Kalman Filter\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
